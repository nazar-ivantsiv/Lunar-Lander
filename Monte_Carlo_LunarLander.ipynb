{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo LunarLander\n",
    "\n",
    "Tabular Monte Carlo method to solve OpenAI GYM LunarLander-v2: https://gym.openai.com/envs/LunarLander-v2/\n",
    "\n",
    "You can find more about this algorithm in Sutton's book Reinforcement Learning: An introduction (Chapter 5) http://incompleteideas.net/book/the-book-2nd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env():\n",
    "    return gym.make('LunarLander-v2')\n",
    "    #env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_preview(env):\n",
    "    env.reset()\n",
    "    for dummy in range(100):\n",
    "        env.render()\n",
    "        state, reward, done, info = env.step(env.action_space.sample())\n",
    "        if done:\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_preview(create_env())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_action_and_env_space(env):\n",
    "    # Action space and environment space\n",
    "    print(\"env.action_space\", env.action_space)\n",
    "    print(\"env.observation_space\", env.observation_space)\n",
    "    print(\"env.observation_space.high\", env.observation_space.high)\n",
    "    print(\"env.observation_space.low\", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buckets\n",
    "We need to discretize the environment values (observation vector); to do this, we are going to use buckets for each dimension of the observation vector. If the real value of the first dimension of the observation vector is in $[a,b]$ and if the number of buckets for that dimension is $n$, we are going to interpolate that real value into an integer in $[0,n-1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_buckets_and_actions():\n",
    "    number_of_buckets = (5,5,5,5,5,5,2,2) #buckets in each dimension\n",
    "    number_of_actions = env.action_space.n\n",
    "    \n",
    "    #Creating a 2-tuple with the original bounds of each dimension\n",
    "    state_value_bounds = list(zip(env.observation_space.low,env.observation_space.high))\n",
    "    \n",
    "    #New bound values for each dimension\n",
    "    state_value_bounds[0] = [-1,1]      #Position x\n",
    "    state_value_bounds[1] = [-1,1]    #Position y\n",
    "    state_value_bounds[2] = [-1,1]        #vel x\n",
    "    state_value_bounds[3] = [-1,1]    #vel y\n",
    "    state_value_bounds[4] = [-1,1]        #angle\n",
    "    state_value_bounds[5] = [-1,1]        #angular vel\n",
    "    state_value_bounds[6] = [0,1]\n",
    "    state_value_bounds[7] = [0,1]\n",
    "    \n",
    "    return number_of_buckets, number_of_actions, state_value_bounds\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env()\n",
    "number_of_buckets = (5,5,5,5,5,5,2,2) #buckets in each dimension\n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "#Creating a 2-tuple with the original bounds of each dimension\n",
    "state_value_bounds = list(zip(env.observation_space.low,env.observation_space.high))\n",
    "\n",
    "#New bound values for each dimension\n",
    "state_value_bounds[0] = [-1,1]      #Position x\n",
    "state_value_bounds[1] = [-1,1]    #Position y\n",
    "state_value_bounds[2] = [-1,1]        #vel x\n",
    "state_value_bounds[3] = [-1,1]    #vel y\n",
    "state_value_bounds[4] = [-1,1]        #angle\n",
    "state_value_bounds[5] = [-1,1]        #angular vel\n",
    "state_value_bounds[6] = [0,1]\n",
    "state_value_bounds[7] = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([inf, inf, inf, inf, inf, inf, inf, inf], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [-1, 1], [0, 1], [0, 1]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_value_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucketize(state):\n",
    "    bucket_indexes = []\n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= state_value_bounds[i][0]:\n",
    "            bucket_index = 0\n",
    "        elif state[i] >= state_value_bounds[i][1]:\n",
    "            bucket_index = number_of_buckets[i] - 1\n",
    "        else:\n",
    "            bound_width = state_value_bounds[i][1] - state_value_bounds[i][0]\n",
    "            offset = (number_of_buckets[i]-1) * state_value_bounds[i][0]/bound_width\n",
    "            scaling = (number_of_buckets[i]-1) / bound_width\n",
    "            bucket_index = int(round(scaling*state[i] - offset))\n",
    "        bucket_indexes.append(bucket_index)\n",
    "    return tuple(bucket_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table():\n",
    "    return np.zeros(number_of_buckets + (number_of_actions,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Returns_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_visits_table():\n",
    "    return np.zeros(number_of_buckets + (number_of_actions,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_function(episode):\n",
    "    return max(min_epsilon, min(max_epsilon, 1.0 - \n",
    "                              math.log10((episode + 1) / (total_train_episodes*0.1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(q_table, bucket_state, epsilon):\n",
    "    if (np.random.random() <= epsilon):\n",
    "        #print(\"random\")\n",
    "        return env.action_space.sample() #Exploration\n",
    "    else:\n",
    "        #print(\"greedy\")\n",
    "        return np.argmax(q_table[bucket_state]) #Eplotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* Generate an episode following pi: S0, A0, R1, ...S_T-1,A_T-1,R_T\n",
    "def Generate_episode(epsilon, q_table, max_env_steps):\n",
    "    # Control variables\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "        \n",
    "    trayectory = []\n",
    "        \n",
    "    # Initialize S\n",
    "    # Reset the environment getting the initial state\n",
    "    bucket_state = bucketize(env.reset())\n",
    "\n",
    "    # Loop for each step of episode:\n",
    "    for step in range(max_env_steps):\n",
    "            #print(\"step \", step)\n",
    "\n",
    "        # Choose A from S using a soft policy derived from Q (e.g., epsilon-greedy)\n",
    "        action = choose_action(q_table, bucket_state, epsilon)\n",
    "            #print(q_table[bucket_state])\n",
    "            #print(\"action \", action)\n",
    "\n",
    "        # Take the action A, observe R, S'\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        bucket_new_state = bucketize(new_state)\n",
    "            #print(\"reward \", reward)\n",
    "            \n",
    "        trayectory.append([bucket_state, action, reward])\n",
    "            \n",
    "        # new_state is now the current state\n",
    "        bucket_state = bucket_new_state\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        # if done, finish the episode\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return trayectory, total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Monte Carlo algorithm\n",
    "\n",
    "* Initialize $Q(s,a) \\in \\mathbb{R}$ arbitrarily ($Q(S,A) = 0$, for all $S,A$ in this case)\n",
    "* $Returns(s,a) \\longleftarrow$ empty list for all $S, A$\n",
    "* Loop for each episode:\n",
    "    * Generate an episode following $\\pi$: $S_0, A_0, R_1, ..., S_{T-1}, A_{T-1}, R_T$\n",
    "    * $G \\longleftarrow 0$\n",
    "    * Loop for each step of episode, $t=T-1, T-2, ..., 0$:\n",
    "        * $G \\longleftarrow \\gamma G + R_{t+1}$\n",
    "        * Unless the pair $S_t, A_t$ appears in $S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}$:\n",
    "            * Append $G$ to $Returns(S_t,A_t)$\n",
    "            * $Q(S_t,A_t)$ $\\longleftarrow$ average($Returns(S_t,A_t)$)\n",
    "            * $A* \\longleftarrow argmax_aQ(S_t,a)$\n",
    "            * For all $a \\in A(S_t)$:\n",
    "                * if $a = A*: \\pi (a|S_t) \\longleftarrow 1 - \\epsilon + \\epsilon / A(S_t)$\n",
    "                * else if $a \\neq A*: \\pi (a|S_t) \\longleftarrow \\epsilon / A(S_t)$\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Monte_Carlo():\n",
    "    # ******* Initialize 𝑄(s,a) arbitrarily.\n",
    "    q_table = initialize_q_table()\n",
    "    #print(\"Q_Table shape: \", q_table.shape)\n",
    "    \n",
    "    # ******* Initialize Returns(s,a) empty list\n",
    "    # initialize visits_counter instead (for incremental implementation of the average)\n",
    "    visits_counter = initialize_visits_table()\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    max_env_steps = env._max_episode_steps   #1000 in LunarLander\n",
    "    #print(\"Max env steps: \", max_env_steps)\n",
    "    \n",
    "\n",
    "    epsilon_tmp = 1.0\n",
    "    epsilon_decay = .996\n",
    "    \n",
    "    # ******* Loop for each episode:\n",
    "    for episode in range(total_train_episodes):\n",
    "        #print(\"\\n\\n ***Episode*** \", episode)\n",
    "        \n",
    "        # ******* Generate an episode following pi: S0, A0, R1, ...S_T-1,A_T-1,R_T\n",
    "        #Update epsilon\n",
    "        epsilon = decay_function(episode)\n",
    "        \n",
    "        \n",
    "        trayectory ,total_reward = Generate_episode(epsilon, q_table, max_env_steps)\n",
    "        \n",
    "        epsilon_tmp *= (epsilon_decay ** len(trayectory))\n",
    "        \n",
    "#         print(\"Epsilon \", epsilon)\n",
    "#         print(\"Epsilon_tmp \", epsilon_tmp)\n",
    "        \n",
    "        # ******* G <-- 0\n",
    "        G = 0\n",
    "        \n",
    "        \n",
    "        # ******* Loop for each step of episode: t = T-1, T-2, ..., 0\n",
    "        for t in reversed(range(len(trayectory))):\n",
    "            #print(\"\\n step\", t)\n",
    "            s_t, a_t, r_t = trayectory[t]\n",
    "            # ******* G <-- gamma*G + R_{t+1}\n",
    "            G = gamma*G + r_t\n",
    "            #print(\"G \", G)\n",
    "            \n",
    "            # ******* Unless the pair S_t,A_t appears in S_0,A_0,R_1, ...,S_{t-1},A_{t-1}: \n",
    "            if not [s_t, a_t] in [[x[0], x[1]] for x in trayectory[0:t]]:\n",
    "                #print(\"YES First visit \", s_t, a_t)\n",
    "                \n",
    "                # ******* Append G to Returns(S_t,A_t)\n",
    "                # ******* Q(S_t,A_t) <-- average(Returns(S_t,A_t))\n",
    "                # Using incremental implementation: Q(S_t,A_t)= Q_n <-- Q_n + (1/n)*(G_n - Q_n)\n",
    "                visits_counter[s_t][a_t] += 1\n",
    "                #print(\"visits_counter \", visits_counter[s_t][a_t])\n",
    "                #print(\"old Q value \", q_table[s_t][a_t])\n",
    "                q_table[s_t][a_t] += (G - q_table[s_t][a_t]) / visits_counter[s_t][a_t]\n",
    "                #print(\"new Q value \", q_table[s_t][a_t])\n",
    "            #else: print(\"NO first visit \", s_t, a_t)\n",
    "        \n",
    "                  \n",
    "        #print(\"total_reward \", total_reward)\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            rewards.append(total_reward)\n",
    "            print(\"Episode {}, epsilon {:5.4f}, reward {:6.2f}\".format(episode,epsilon,total_reward))  \n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    print(\"Episode {}, epsilon {:5.4f}, reward {:6.2f}\".format(episode,epsilon,total_reward))\n",
    "    return q_table, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Q-Learning training plots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Q-Learning algorithm n times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********** Training number  0\n",
      "Episode 0, epsilon 1.0000, reward -123.60\n",
      "Episode 50, epsilon 1.0000, reward -112.02\n",
      "Episode 100, epsilon 1.0000, reward -20.60\n",
      "Episode 150, epsilon 1.0000, reward -415.83\n",
      "Episode 200, epsilon 1.0000, reward -115.62\n",
      "Episode 250, epsilon 1.0000, reward -257.44\n",
      "Episode 300, epsilon 1.0000, reward -82.78\n",
      "Episode 350, epsilon 1.0000, reward -186.14\n",
      "Episode 400, epsilon 1.0000, reward -89.05\n",
      "Episode 450, epsilon 1.0000, reward -304.17\n",
      "Episode 500, epsilon 1.0000, reward -121.66\n",
      "Episode 550, epsilon 1.0000, reward -159.54\n",
      "Episode 600, epsilon 1.0000, reward -298.11\n",
      "Episode 650, epsilon 1.0000, reward -121.31\n",
      "Episode 700, epsilon 1.0000, reward -136.39\n",
      "Episode 750, epsilon 1.0000, reward -122.12\n",
      "Episode 800, epsilon 1.0000, reward -108.50\n",
      "Episode 850, epsilon 1.0000, reward -351.01\n",
      "Episode 900, epsilon 1.0000, reward -119.22\n",
      "Episode 950, epsilon 1.0000, reward -199.92\n",
      "Episode 1000, epsilon 0.9996, reward -102.05\n",
      "Episode 1050, epsilon 0.9784, reward -304.15\n",
      "Episode 1100, epsilon 0.9582, reward -324.84\n",
      "Episode 1150, epsilon 0.9389, reward -111.08\n",
      "Episode 1200, epsilon 0.9205, reward -271.38\n",
      "Episode 1250, epsilon 0.9027, reward -77.47\n",
      "Episode 1300, epsilon 0.8857, reward -102.56\n",
      "Episode 1350, epsilon 0.8693, reward -67.06\n",
      "Episode 1400, epsilon 0.8536, reward -114.30\n",
      "Episode 1450, epsilon 0.8383, reward -127.80\n",
      "Episode 1500, epsilon 0.8236, reward -124.80\n",
      "Episode 1550, epsilon 0.8094, reward -248.41\n",
      "Episode 1600, epsilon 0.7956, reward -78.10\n",
      "Episode 1650, epsilon 0.7823, reward -220.43\n",
      "Episode 1700, epsilon 0.7693, reward -73.70\n",
      "Episode 1750, epsilon 0.7567, reward -86.26\n",
      "Episode 1800, epsilon 0.7445, reward -33.55\n",
      "Episode 1850, epsilon 0.7326, reward -69.22\n",
      "Episode 1900, epsilon 0.7210, reward -93.40\n",
      "Episode 1950, epsilon 0.7097, reward -125.50\n",
      "Episode 2000, epsilon 0.6988, reward -73.12\n",
      "Episode 2050, epsilon 0.6880, reward -113.70\n",
      "Episode 2100, epsilon 0.6776, reward -226.29\n",
      "Episode 2150, epsilon 0.6674, reward -22.61\n",
      "Episode 2200, epsilon 0.6574, reward -170.29\n",
      "Episode 2250, epsilon 0.6476, reward -108.04\n",
      "Episode 2300, epsilon 0.6381, reward -79.28\n",
      "Episode 2350, epsilon 0.6287, reward -46.35\n",
      "Episode 2400, epsilon 0.6196, reward -71.00\n",
      "Episode 2450, epsilon 0.6107, reward -68.74\n",
      "Episode 2500, epsilon 0.6019, reward -40.92\n",
      "Episode 2550, epsilon 0.5933, reward -125.19\n",
      "Episode 2600, epsilon 0.5849, reward  -4.36\n",
      "Episode 2650, epsilon 0.5766, reward -12.55\n",
      "Episode 2700, epsilon 0.5685, reward -157.86\n",
      "Episode 2750, epsilon 0.5605, reward -85.27\n",
      "Episode 2800, epsilon 0.5527, reward -76.28\n",
      "Episode 2850, epsilon 0.5450, reward -77.09\n",
      "Episode 2900, epsilon 0.5375, reward -49.07\n",
      "Episode 2950, epsilon 0.5300, reward -110.86\n",
      "Episode 3000, epsilon 0.5227, reward -97.42\n",
      "Episode 3050, epsilon 0.5156, reward -104.54\n",
      "Episode 3100, epsilon 0.5085, reward -52.58\n",
      "Episode 3150, epsilon 0.5016, reward -58.37\n",
      "Episode 3200, epsilon 0.4947, reward -87.36\n",
      "Episode 3250, epsilon 0.4880, reward -249.25\n",
      "Episode 3300, epsilon 0.4814, reward -81.15\n",
      "Episode 3350, epsilon 0.4748, reward -33.03\n",
      "Episode 3400, epsilon 0.4684, reward -473.30\n",
      "Episode 3450, epsilon 0.4621, reward -111.67\n",
      "Episode 3500, epsilon 0.4558, reward -76.39\n",
      "Episode 3550, epsilon 0.4496, reward -37.53\n",
      "Episode 3600, epsilon 0.4436, reward -46.15\n",
      "Episode 3650, epsilon 0.4376, reward -16.70\n",
      "Episode 3700, epsilon 0.4317, reward  22.34\n",
      "Episode 3750, epsilon 0.4259, reward -69.60\n",
      "Episode 3800, epsilon 0.4201, reward -28.14\n",
      "Episode 3850, epsilon 0.4144, reward -69.09\n",
      "Episode 3900, epsilon 0.4088, reward -49.70\n",
      "Episode 3950, epsilon 0.4033, reward -20.18\n",
      "Episode 4000, epsilon 0.3978, reward -24.55\n",
      "Episode 4050, epsilon 0.3924, reward -23.72\n",
      "Episode 4100, epsilon 0.3871, reward -172.21\n",
      "Episode 4150, epsilon 0.3818, reward -32.81\n",
      "Episode 4200, epsilon 0.3766, reward -50.07\n",
      "Episode 4250, epsilon 0.3715, reward -42.58\n",
      "Episode 4300, epsilon 0.3664, reward -42.07\n",
      "Episode 4350, epsilon 0.3614, reward -190.80\n",
      "Episode 4400, epsilon 0.3564, reward  -0.77\n",
      "Episode 4450, epsilon 0.3515, reward -21.28\n",
      "Episode 4500, epsilon 0.3467, reward -10.64\n",
      "Episode 4550, epsilon 0.3419, reward -37.11\n",
      "Episode 4600, epsilon 0.3371, reward -214.80\n",
      "Episode 4650, epsilon 0.3325, reward -52.06\n",
      "Episode 4700, epsilon 0.3278, reward  22.98\n",
      "Episode 4750, epsilon 0.3232, reward -17.50\n",
      "Episode 4800, epsilon 0.3187, reward   1.30\n",
      "Episode 4850, epsilon 0.3142, reward  49.35\n",
      "Episode 4900, epsilon 0.3097, reward -26.40\n",
      "Episode 4950, epsilon 0.3053, reward  12.95\n",
      "Episode 5000, epsilon 0.3009, reward  40.95\n",
      "Episode 5050, epsilon 0.2966, reward -72.63\n",
      "Episode 5100, epsilon 0.2923, reward -13.57\n",
      "Episode 5150, epsilon 0.2881, reward -47.60\n",
      "Episode 5200, epsilon 0.2839, reward -34.03\n",
      "Episode 5250, epsilon 0.2798, reward -30.42\n",
      "Episode 5300, epsilon 0.2756, reward  34.30\n",
      "Episode 5350, epsilon 0.2716, reward -76.73\n",
      "Episode 5400, epsilon 0.2675, reward -49.25\n",
      "Episode 5450, epsilon 0.2635, reward -46.04\n",
      "Episode 5500, epsilon 0.2596, reward -19.15\n",
      "Episode 5550, epsilon 0.2556, reward   9.49\n",
      "Episode 5600, epsilon 0.2517, reward  20.54\n",
      "Episode 5650, epsilon 0.2479, reward -62.89\n",
      "Episode 5700, epsilon 0.2440, reward   0.92\n",
      "Episode 5750, epsilon 0.2403, reward  26.29\n",
      "Episode 5800, epsilon 0.2365, reward -10.97\n",
      "Episode 5850, epsilon 0.2328, reward -58.08\n",
      "Episode 5900, epsilon 0.2291, reward -31.89\n",
      "Episode 5950, epsilon 0.2254, reward  -7.09\n",
      "Episode 6000, epsilon 0.2218, reward -18.81\n",
      "Episode 6050, epsilon 0.2182, reward -49.05\n",
      "Episode 6100, epsilon 0.2146, reward -82.55\n",
      "Episode 6150, epsilon 0.2111, reward -12.34\n",
      "Episode 6200, epsilon 0.2075, reward -52.26\n",
      "Episode 6250, epsilon 0.2041, reward -34.03\n",
      "Episode 6300, epsilon 0.2006, reward -51.91\n",
      "Episode 6350, epsilon 0.1972, reward   3.24\n",
      "Episode 6400, epsilon 0.1938, reward  28.22\n",
      "Episode 6450, epsilon 0.1904, reward -51.58\n",
      "Episode 6500, epsilon 0.1870, reward 256.17\n",
      "Episode 6550, epsilon 0.1837, reward -62.84\n",
      "Episode 6600, epsilon 0.1804, reward -54.07\n",
      "Episode 6650, epsilon 0.1771, reward -51.94\n",
      "Episode 6700, epsilon 0.1739, reward  -8.35\n",
      "Episode 6750, epsilon 0.1706, reward  18.86\n",
      "Episode 6800, epsilon 0.1674, reward 281.10\n",
      "Episode 6850, epsilon 0.1642, reward  23.17\n",
      "Episode 6900, epsilon 0.1611, reward   0.20\n",
      "Episode 6950, epsilon 0.1580, reward  22.58\n",
      "Episode 7000, epsilon 0.1548, reward   9.26\n",
      "Episode 7050, epsilon 0.1517, reward -72.70\n",
      "Episode 7100, epsilon 0.1487, reward -10.20\n",
      "Episode 7150, epsilon 0.1456, reward 266.28\n",
      "Episode 7200, epsilon 0.1426, reward   1.19\n",
      "Episode 7250, epsilon 0.1396, reward   0.36\n",
      "Episode 7300, epsilon 0.1366, reward 264.62\n",
      "Episode 7350, epsilon 0.1337, reward -74.30\n",
      "Episode 7400, epsilon 0.1307, reward  41.07\n",
      "Episode 7450, epsilon 0.1278, reward -35.32\n",
      "Episode 7500, epsilon 0.1249, reward -50.64\n",
      "Episode 7550, epsilon 0.1220, reward 210.79\n",
      "Episode 7600, epsilon 0.1191, reward  -8.20\n",
      "Episode 7650, epsilon 0.1163, reward  14.26\n",
      "Episode 7700, epsilon 0.1135, reward 194.54\n",
      "Episode 7750, epsilon 0.1106, reward   1.53\n",
      "Episode 7800, epsilon 0.1078, reward -37.69\n",
      "Episode 7850, epsilon 0.1051, reward -73.33\n",
      "Episode 7900, epsilon 0.1023, reward  -2.30\n",
      "Episode 7950, epsilon 0.0996, reward   7.39\n",
      "Episode 8000, epsilon 0.0969, reward -41.70\n",
      "Episode 8050, epsilon 0.0942, reward 103.05\n",
      "Episode 8100, epsilon 0.0915, reward -61.47\n",
      "Episode 8150, epsilon 0.0888, reward  13.35\n",
      "Episode 8200, epsilon 0.0861, reward  -1.07\n",
      "Episode 8250, epsilon 0.0835, reward  10.45\n",
      "Episode 8300, epsilon 0.0809, reward 203.60\n",
      "Episode 8350, epsilon 0.0783, reward 254.10\n",
      "Episode 8400, epsilon 0.0757, reward -17.12\n",
      "Episode 8450, epsilon 0.0731, reward  19.67\n",
      "Episode 8500, epsilon 0.0705, reward 228.68\n",
      "Episode 8550, epsilon 0.0680, reward -20.97\n",
      "Episode 8600, epsilon 0.0655, reward -48.31\n",
      "Episode 8650, epsilon 0.0629, reward -80.38\n",
      "Episode 8700, epsilon 0.0604, reward  11.54\n",
      "Episode 8750, epsilon 0.0579, reward -16.59\n",
      "Episode 8800, epsilon 0.0555, reward -12.49\n",
      "Episode 8850, epsilon 0.0530, reward  -5.03\n",
      "Episode 8900, epsilon 0.0506, reward 258.75\n",
      "Episode 8950, epsilon 0.0481, reward 254.72\n",
      "Episode 9000, epsilon 0.0457, reward 252.83\n",
      "Episode 9050, epsilon 0.0433, reward 251.85\n",
      "Episode 9100, epsilon 0.0409, reward -56.88\n",
      "Episode 9150, epsilon 0.0385, reward  22.05\n",
      "Episode 9200, epsilon 0.0362, reward 245.70\n",
      "Episode 9250, epsilon 0.0338, reward  13.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9300, epsilon 0.0315, reward -10.10\n",
      "Episode 9350, epsilon 0.0291, reward -38.87\n",
      "Episode 9400, epsilon 0.0268, reward  -9.74\n",
      "Episode 9450, epsilon 0.0245, reward 261.35\n",
      "Episode 9500, epsilon 0.0222, reward -56.00\n",
      "Episode 9550, epsilon 0.0200, reward  32.66\n",
      "Episode 9600, epsilon 0.0177, reward   3.80\n",
      "Episode 9650, epsilon 0.0154, reward   1.56\n",
      "Episode 9700, epsilon 0.0132, reward 299.53\n",
      "Episode 9750, epsilon 0.0110, reward 255.64\n",
      "Episode 9800, epsilon 0.0100, reward  -6.76\n",
      "Episode 9850, epsilon 0.0100, reward 263.39\n",
      "Episode 9900, epsilon 0.0100, reward 244.83\n",
      "Episode 9950, epsilon 0.0100, reward 278.98\n",
      "Episode 9999, epsilon 0.0100, reward 271.12\n"
     ]
    }
   ],
   "source": [
    "# n_times = 5\n",
    "n_times = 1\n",
    "\n",
    "total_train_episodes = 10000\n",
    "gamma = 0.99                     \n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01   \n",
    "\n",
    "env = create_env()\n",
    "#env_preview(env)\n",
    "#show_action_and_env_space(env)\n",
    "number_of_buckets, number_of_actions, state_value_bounds = set_buckets_and_actions()\n",
    "\n",
    "MC_tables = []\n",
    "MC_rewards = []\n",
    "\n",
    "for number in range(n_times):\n",
    "    print(\"\\n ********** Training number \", number)\n",
    "    q_table,rewards = Monte_Carlo()\n",
    "    MC_tables.append(q_table)\n",
    "    MC_rewards.append(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trayectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving average reward and Q-tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_mean = np.mean(np.array(MC_rewards),axis=0)\n",
    "#print(MC_mean.shape)\n",
    "np.save('MC_mean', MC_mean)\n",
    "np.save('MC_tables', MC_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Q-Learning rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "\n",
    "x = np.linspace(0, total_train_episodes, MC_mean.size)\n",
    "plt.plot(x, MC_mean, label='Monte Carlo')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Q(s,·) values for a given initial state s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State  (2, 4, 2, 3, 2, 2, 0, 0)\n",
      "Q(s,·)  [-19.46455431 -44.51107605 -45.20502187 -40.81539612]\n"
     ]
    }
   ],
   "source": [
    "#env = create_env()\n",
    "q_tables = np.load('MC_tables.npy')\n",
    "q_table = q_tables[0]\n",
    "\n",
    "number_of_buckets, number_of_actions, state_value_bounds = set_buckets_and_actions()\n",
    "bucket_state = bucketize(env.reset())\n",
    "\n",
    "print(\"State \", bucket_state)\n",
    "print(\"Q(s,·) \", q_table[bucket_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056752874862138314"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = 350\n",
    "total = 400\n",
    "e_min = 0.01\n",
    "e_max = 1.0\n",
    "np.clip(1.0 - np.log10((e + 1) / (total * 0.1)), e_min, e_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing LunarLander with the learned Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score  255.1127841228271\n",
      "Score  252.47205871637095\n",
      "Score  11.964253846899084\n",
      "Score  -26.488068170885626\n",
      "Score  274.0721995183501\n",
      "Score  267.0852608674311\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2bb3b305554b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# *******Loop for each step of episode:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_env_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#******* Choose A from S using policy derived from Q (greedy in this case)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lunar_lander/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lunar_lander/lib/python3.7/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    374\u001b[0m                                      color=(0.8, 0.8, 0))\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lunar_lander/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mglClearColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lunar_lander/lib/python3.7/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactive\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \"\"\"\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_COLOR_BUFFER_BIT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_DEPTH_BUFFER_BIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#env = create_env()\n",
    "total_test_episodes = 10\n",
    "q_tables = np.load('MC_tables.npy')\n",
    "q_table = q_tables[0]\n",
    "rewards = []\n",
    "max_env_steps = env._max_episode_steps\n",
    "number_of_buckets, number_of_actions, state_value_bounds = set_buckets_and_actions()\n",
    "\n",
    "# ******* Loop for each episode:\n",
    "for episode in range(total_test_episodes):\n",
    "    #print(\"***Episode*** \", episode)\n",
    "    \n",
    "    # Control variables\n",
    "    total_rewards = 0\n",
    "    done =  False\n",
    "    \n",
    "    # ******* Initialize S\n",
    "    # Reset the environment getting the initial state\n",
    "    bucket_state = bucketize(env.reset())\n",
    "    \n",
    "    # *******Loop for each step of episode:\n",
    "    for step in range(max_env_steps):\n",
    "        env.render()\n",
    "        \n",
    "        #******* Choose A from S using policy derived from Q (greedy in this case)\n",
    "        action = np.argmax(q_table[bucket_state])\n",
    "        #print(action)\n",
    "        \n",
    "        # ******* Take the action A, observe R, S'\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        bucket_new_state = bucketize(new_state)\n",
    "        \n",
    "        # new_state is now the current state\n",
    "        bucket_state =  bucket_new_state\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            print(\"Score \", total_rewards)\n",
    "            break\n",
    "                \n",
    "env.close()\n",
    "print(\"\\nAverage score \" + str(sum(rewards)/total_test_episodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
